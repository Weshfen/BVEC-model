{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042dbf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ef26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NTL imagery and building volume data to construct the comprehensive dataset\n",
    "image_paths = [\n",
    "    r'NTL.tif'\n",
    "]\n",
    "label_paths = [\n",
    "    r'building_volume.tif'\n",
    "]\n",
    "\n",
    "LM_list = []\n",
    "label_list = []\n",
    "\n",
    "for image_path, label_path in zip(image_paths, label_paths):\n",
    "    image_A = tifffile.imread(image_path)\n",
    "    \n",
    "    if image_A.shape[0] % 2 != 0:\n",
    "        image_A = np.pad(image_A, ((0, 1), (0, 0)), mode='constant')\n",
    "    if image_A.shape[1] % 2 != 0:\n",
    "        image_A = np.pad(image_A, ((0, 0), (0, 1)), mode='constant')\n",
    "    BM = torch.from_numpy(image_A).unsqueeze(0).unsqueeze(0).float()\n",
    "    padding = 8\n",
    "    padded_image_A = np.pad(image_A, ((padding, padding), (padding, padding)), mode='constant')\n",
    "\n",
    "    crop_size = 18\n",
    "    stride = 1\n",
    "    num_crops = ((padded_image_A.shape[0] - crop_size) // stride + 1) * ((padded_image_A.shape[1] - crop_size) // stride + 1)\n",
    "    LM = torch.zeros(num_crops, 1, crop_size, crop_size)\n",
    "\n",
    "    # slide cropping\n",
    "    crop_idx = 0\n",
    "    for i in range(0, padded_image_A.shape[0] - crop_size + 1, stride):\n",
    "        for j in range(0, padded_image_A.shape[1] - crop_size + 1, stride):\n",
    "            crop = padded_image_A[i:i+crop_size, j:j+crop_size]\n",
    "            LM[crop_idx, 0, :, :] = torch.from_numpy(crop)\n",
    "            crop_idx += 1\n",
    "\n",
    "    image_B = tifffile.imread(label_path)\n",
    "\n",
    "    if image_B.shape[0] % 2 != 0:\n",
    "        image_B = np.pad(image_B, ((0, 1), (0, 0)), mode='constant')\n",
    "    if image_B.shape[1] % 2 != 0:\n",
    "        image_B = np.pad(image_B, ((0, 0), (0, 1)), mode='constant')\n",
    "\n",
    "    BL = torch.from_numpy(image_B).unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "    crop_size_B = 2\n",
    "    stride_B = 1\n",
    "    num_crops_B = ((image_B.shape[0] - crop_size_B) // stride_B + 1) * ((image_B.shape[1] - crop_size_B) // stride_B + 1)\n",
    "    LM_B = torch.zeros(num_crops_B, 1, crop_size_B, crop_size_B)\n",
    "\n",
    "    # slide cropping\n",
    "    crop_idx_B = 0\n",
    "    for i in range(0, image_B.shape[0] - crop_size_B + 1, stride_B):\n",
    "        for j in range(0, image_B.shape[1] - crop_size_B + 1, stride_B):\n",
    "            crop_B = image_B[i:i+crop_size_B, j:j+crop_size_B]\n",
    "            LM_B[crop_idx_B, 0, :, :] = torch.from_numpy(crop_B)\n",
    "            crop_idx_B += 1\n",
    "\n",
    "    label_tensor = LM_B.view(num_crops_B, -1).sum(dim=1).unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "    LM_list.append(LM)\n",
    "    label_list.append(label_tensor)\n",
    "\n",
    "combined_LM = torch.cat(LM_list, dim=0)\n",
    "combined_label = torch.cat(label_list, dim=0)\n",
    "\n",
    "LM = combined_LM\n",
    "combined_label = torch.squeeze(combined_label, dim=( 0, 2, 3)) \n",
    "label_tensor = combined_label\n",
    "\n",
    "keep_indices = []\n",
    "\n",
    "for i in range(len(LM)):\n",
    "    if torch.any(LM[i] != 0) and torch.any(label_tensor[i] != 0):\n",
    "        keep_indices.append(i)\n",
    "        \n",
    "new_LM = LM[keep_indices]\n",
    "new_label_tensor = label_tensor[keep_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aabbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# division of different sample clusters\n",
    "sum_values = new_LM.view(-1, 18*18).sum(dim=1)\n",
    "\n",
    "NPC_A = []\n",
    "NPC_B = []\n",
    "NPC_C = []\n",
    "\n",
    "for i, value in enumerate(sum_values):\n",
    "    if value > 9300:\n",
    "        NPC_A.append(i)\n",
    "    elif 3600 <= value <= 9300:\n",
    "        NPC_B.append(i)\n",
    "    else:\n",
    "        NPC_C.append(i)\n",
    "\n",
    "new_LMA = new_LM[NPC_A]\n",
    "new_LMB = new_LM[NPC_B]\n",
    "new_LMC = new_LM[NPC_C]\n",
    "\n",
    "new_label_tensorA = new_label_tensor[NPC_A]\n",
    "new_label_tensorB = new_label_tensor[NPC_B]\n",
    "new_label_tensorC = new_label_tensor[NPC_C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training and test dataset\n",
    "step_size = 6\n",
    "\n",
    "dataset1_size = new_LMA.size(0)\n",
    "dataset2_size = new_LMB.size(0)\n",
    "dataset3_size = new_LMC.size(0)\n",
    "\n",
    "Test_Ax = new_LMA[::step_size]\n",
    "Test_Ay = new_label_tensorA[::step_size]\n",
    "Test_Bx = new_LMB[::step_size]\n",
    "Test_By = new_label_tensorB[::step_size]\n",
    "Test_Cx = new_LMC[::step_size]\n",
    "Test_Cy = new_label_tensorC[::step_size]\n",
    "\n",
    "Train_Ax = torch.cat([new_LMA[i].unsqueeze(0) for i in range(dataset1_size) if i % step_size != 0])\n",
    "Train_Ay = torch.cat([new_label_tensorA[i].unsqueeze(0) for i in range(dataset1_size) if i % step_size != 0])\n",
    "Train_Bx = torch.cat([new_LMB[i].unsqueeze(0) for i in range(dataset2_size) if i % step_size != 0])\n",
    "Train_By = torch.cat([new_label_tensorB[i].unsqueeze(0) for i in range(dataset2_size) if i % step_size != 0])\n",
    "Train_Cx = torch.cat([new_LMC[i].unsqueeze(0) for i in range(dataset3_size) if i % step_size != 0])\n",
    "Train_Cy = torch.cat([new_label_tensorC[i].unsqueeze(0) for i in range(dataset3_size) if i % step_size != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f922bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data enhancement\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "flip_transform = transforms.RandomHorizontalFlip()\n",
    "vertical_flip_transform = transforms.RandomVerticalFlip()\n",
    "\n",
    "augmented_dataA = []\n",
    "augmented_dataB = []\n",
    "augmented_dataC = []\n",
    "\n",
    "for i in range(len(Train_Ax)):\n",
    "    flipped_sampleA = flip_transform(Train_Ax[i])\n",
    "    labelA = Train_Ay[i]\n",
    "    augmented_dataA.append((flipped_sampleA, labelA))\n",
    "for i in range(len(Train_Bx)):\n",
    "    flipped_sampleB = flip_transform(Train_Bx[i])\n",
    "    labelB = Train_By[i]\n",
    "    augmented_dataB.append((flipped_sampleB, labelB))    \n",
    "for i in range(len(Train_Ax)):\n",
    "    flipped_sampleC = flip_transform(Train_Cx[i])\n",
    "    labelC = Train_Cy[i]\n",
    "    augmented_dataC.append((flipped_sampleC, labelC))\n",
    "\n",
    "for i in range(len(Train_Ax)):\n",
    "    flipped_sample = vertical_flip_transform(Train_Ax[i])\n",
    "    labelA = Train_Ay[i]\n",
    "    augmented_dataA.append((flipped_sample, labelA))\n",
    "for i in range(len(Train_Bx)):\n",
    "    flipped_sampleB = vertical_flip_transform(Train_Bx[i])\n",
    "    labelB = Train_By[i]\n",
    "    augmented_dataB.append((flipped_sampleB, labelB))    \n",
    "for i in range(len(Train_Ax)):\n",
    "    flipped_sampleC = vertical_flip_transform(Train_Cx[i])\n",
    "    labelC = Train_Cy[i]\n",
    "    augmented_dataC.append((flipped_sampleC, labelC))\n",
    "\n",
    "combined_dataA = augmented_dataA + [(Train_Ax[i], Train_Ay[i]) for i in range(len(Train_Ax))]\n",
    "combined_dataB = augmented_dataB + [(Train_Bx[i], Train_By[i]) for i in range(len(Train_Bx))]\n",
    "combined_dataC = augmented_dataC + [(Train_Cx[i], Train_Cy[i]) for i in range(len(Train_Cx))]\n",
    "\n",
    "combined_LMA = torch.stack([sample for sample, _ in combined_dataA])\n",
    "combined_label_tensorA = torch.stack([label for _, label in combined_dataA])\n",
    "combined_LMB = torch.stack([sample for sample, _ in combined_dataB])\n",
    "combined_label_tensorB = torch.stack([label for _, label in combined_dataB])\n",
    "combined_LMC = torch.stack([sample for sample, _ in combined_dataC])\n",
    "combined_label_tensorC = torch.stack([label for _, label in combined_dataC])\n",
    "\n",
    "combined_datasetA = TensorDataset(combined_LMA, combined_label_tensorA)\n",
    "combined_datasetB = TensorDataset(combined_LMB, combined_label_tensorB)\n",
    "combined_datasetC = TensorDataset(combined_LMC, combined_label_tensorC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing neural network models\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 300\n",
    "LR = 0.01\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# data loading\n",
    "data_loaderA = DataLoader(combined_datasetA, batch_size=BATCH_SIZE, shuffle=True) \n",
    "\n",
    "train_sizeA = int(0.8 * len(combined_datasetA))\n",
    "test_sizeA = len(combined_datasetA) - train_sizeA\n",
    "train_datasetA, test_datasetA = torch.utils.data.random_split(combined_datasetA, [train_sizeA, test_sizeA])\n",
    "\n",
    "train_loaderA = DataLoader(train_datasetA, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loaderA = DataLoader(test_datasetA, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "data_loaderB = DataLoader(combined_datasetB, batch_size=BATCH_SIZE, shuffle=True) \n",
    "\n",
    "train_sizeB = int(0.8 * len(combined_datasetB))\n",
    "test_sizeB = len(combined_datasetB) - train_sizeB\n",
    "train_datasetB, test_datasetB = torch.utils.data.random_split(combined_datasetB, [train_sizeB, test_sizeB])\n",
    "\n",
    "train_loaderB = DataLoader(train_datasetB, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loaderB = DataLoader(test_datasetB, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "data_loaderC = DataLoader(combined_datasetC, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_sizeC = int(0.8 * len(combined_datasetC))\n",
    "test_sizeC = len(combined_datasetC) - train_sizeC\n",
    "train_datasetC, test_datasetC = torch.utils.data.random_split(combined_datasetC, [train_sizeC, test_sizeC])\n",
    "\n",
    "train_loaderC = DataLoader(train_datasetC, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loaderC = DataLoader(test_datasetC, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d120967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionNet, self).__init__()        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1)\n",
    "        self.relu1 = nn.ReLU()        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)               \n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, stride=1)\n",
    "        self.relu3 = nn.ReLU()        \n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 128)\n",
    "        self.relu4 = nn.ReLU()        \n",
    "        self.fc2 = nn.Linear(128, 64) \n",
    "        self.relu5 = nn.ReLU()        \n",
    "        self.fc3 = nn.Linear(64, 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)        \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.pool2(out)                      \n",
    "        out = self.conv3(out)\n",
    "        out = self.relu3(out)      \n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# calculating MAPE\n",
    "def calculate_mape(output, target):\n",
    "    absolute_error = torch.abs(output - target)\n",
    "    percentage_error = (absolute_error / target) * 100\n",
    "    mape = torch.mean(percentage_error)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ce018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Model A\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "modelA = RegressionNet().to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(modelA.parameters(), lr=LR, weight_decay=0.005)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    modelA.train()\n",
    "    train_loss = 0.0\n",
    "    train_mape = 0.0\n",
    "    train_mean_difference = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loaderA):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = modelA(data)\n",
    "        loss = criterion(output, target)\n",
    "        mape = calculate_mape(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        train_mape += mape * data.size(0)\n",
    "    train_loss /= len(train_datasetA)\n",
    "    train_mape /= len(train_datasetA)\n",
    "    train_mean_difference /= len(train_datasetA)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    modelA.eval()\n",
    "    test_loss = 0.0\n",
    "    test_mape = 0.0\n",
    "    test_mean_difference = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loaderA:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = modelA(data)\n",
    "            loss = criterion(output, target)\n",
    "            mape = calculate_mape(output, target)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            test_mape += mape * data.size(0)\n",
    "    test_loss /= len(test_datasetA)\n",
    "    test_mape /= len(test_datasetA)\n",
    "    test_mean_difference /= len(test_datasetA)\n",
    "    \n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print('Epoch: {}, Train MAPE: {:.2f}%,Test MAPE: {:.2f}%'.format(epoch, train_mape, test_mape))\n",
    "\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, EPOCHS + 1), test_losses, label='Testing Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Model B\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "modelB = RegressionNet().to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(modelB.parameters(), lr=LR, weight_decay=0.005)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    modelB.train()\n",
    "    train_loss = 0.0\n",
    "    train_mape = 0.0\n",
    "    train_mean_difference = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loaderB):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = modelB(data)\n",
    "        loss = criterion(output, target)\n",
    "        mape = calculate_mape(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        train_mape += mape * data.size(0)\n",
    "    train_loss /= len(train_datasetB)\n",
    "    train_mape /= len(train_datasetB)\n",
    "    train_mean_difference /= len(train_datasetB)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    modelB.eval()\n",
    "    test_loss = 0.0\n",
    "    test_mape = 0.0\n",
    "    test_mean_difference = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loaderB:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = modelB(data)\n",
    "            loss = criterion(output, target)\n",
    "            mape = calculate_mape(output, target)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            test_mape += mape * data.size(0)\n",
    "    test_loss /= len(test_datasetB)\n",
    "    test_mape /= len(test_datasetB)\n",
    "    test_mean_difference /= len(test_datasetB)\n",
    "    \n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print('Epoch: {}, Train MAPE: {:.2f}%,Test MAPE: {:.2f}%'.format(epoch, train_mape, test_mape))\n",
    "    \n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, EPOCHS + 1), test_losses, label='Testing Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Model C\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "modelC = RegressionNet().to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(modelC.parameters(), lr=LR, weight_decay=0.005)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    modelC.train()\n",
    "    train_loss = 0.0\n",
    "    train_mape = 0.0\n",
    "    train_mean_difference = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loaderC):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = modelC(data)\n",
    "        loss = criterion(output, target)\n",
    "        mape = calculate_mape(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        train_mape += mape * data.size(0)\n",
    "    train_loss /= len(train_datasetC)\n",
    "    train_mape /= len(train_datasetC)\n",
    "    train_mean_difference /= len(train_datasetC)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    modelC.eval()\n",
    "    test_loss = 0.0\n",
    "    test_mape = 0.0\n",
    "    test_mean_difference = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loaderC:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = modelC(data)\n",
    "            loss = criterion(output, target)\n",
    "            mape = calculate_mape(output, target)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            test_mape += mape * data.size(0)\n",
    "    test_loss /= len(test_datasetC)\n",
    "    test_mape /= len(test_datasetC)\n",
    "    test_mean_difference /= len(test_datasetC)\n",
    "    \n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print('Epoch: {}, Train MAPE: {:.2f}%,Test MAPE: {:.2f}%'.format(epoch, train_mape, test_mape))\n",
    "\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, EPOCHS + 1), test_losses, label='Testing Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8bbfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Model A\n",
    "modelA.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictionsA = modelA(new_LMA.to(DEVICE))\n",
    "    predictionsTA = modelA(Test_Ax.to(DEVICE))\n",
    "\n",
    "print('overall performance on training set')\n",
    "print(int((torch.sum(predictionsA) - torch.sum(new_label_tensorA))/torch.sum(new_label_tensorA)*100),'%')\n",
    "print('overall performance on test set')\n",
    "print(int((torch.sum(predictionsTA) - torch.sum(Test_Ay))/torch.sum(Test_Ay)*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Model B\n",
    "modelB.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictionsB = modelB(new_LMB.to(DEVICE))\n",
    "    predictionsTB = modelB(Test_Bx.to(DEVICE))\n",
    "\n",
    "print('overall performance on training set')\n",
    "print(int((torch.sum(predictionsB) - torch.sum(new_label_tensorB))/torch.sum(new_label_tensorB)*100),'%')\n",
    "print('overall performance on test set')\n",
    "print(int((torch.sum(predictionsTB) - torch.sum(Test_By))/torch.sum(Test_By)*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e150cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Model C\n",
    "modelC.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictionsC = modelC(new_LMC.to(DEVICE))\n",
    "    predictionsTC = modelC(Test_Cx.to(DEVICE))\n",
    "\n",
    "print('overall performance on training set')\n",
    "print(int((torch.sum(predictionsC) - torch.sum(new_label_tensorC))/torch.sum(new_label_tensorC)*100),'%')\n",
    "print('overall performance on test set')\n",
    "print(int((torch.sum(predictionsTC) - torch.sum(Test_Cy))/torch.sum(Test_Cy)*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a96c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizing the BVEC model for prediction tasks\n",
    "\n",
    "# import nighttime lights data of target areas\n",
    "image_path_CSJ = r'E:\\NTL_CSJ.tif'\n",
    "image_CSJ = tifffile.imread(image_path_CSJ)\n",
    "\n",
    "if image_CSJ.shape[0] % 2 != 0:\n",
    "    image_CSJ = np.pad(image_CSJ, ((0, 1), (0, 0)), mode='constant')\n",
    "if image_CSJ.shape[1] % 2 != 0:\n",
    "    image_CSJ = np.pad(image_CSJ, ((0, 0), (0, 1)), mode='constant')\n",
    "\n",
    "CSJntl = torch.from_numpy(image_CSJ).unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "padding = 8\n",
    "padded_image_CSJ = np.pad(image_CSJ, ((padding, padding), (padding, padding)), mode='constant')\n",
    "\n",
    "crop_size = 18\n",
    "stride = 2\n",
    "num_crops = ((padded_image_CSJ.shape[0] - crop_size) // stride + 1) * ((padded_image_CSJ.shape[1] - crop_size) // stride + 1)\n",
    "CSJntl = torch.zeros(num_crops, 1, crop_size, crop_size)\n",
    "\n",
    "crop_idx = 0\n",
    "for i in range(0, padded_image_CSJ.shape[0] - crop_size + 1, stride):\n",
    "    for j in range(0, padded_image_CSJ.shape[1] - crop_size + 1, stride):\n",
    "        crop = padded_image_CSJ[i:i+crop_size, j:j+crop_size]\n",
    "        CSJntl[crop_idx, 0, :, :] = torch.from_numpy(crop)\n",
    "        crop_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_valu = CSJntl.view(-1, 18*18).sum(dim=1)\n",
    "ntl_A = []\n",
    "ntl_B = []\n",
    "ntl_C = []\n",
    "\n",
    "for i, value in enumerate(sum_valu):\n",
    "    if value > 9300:\n",
    "        ntl_A.append(i)\n",
    "    elif 3600 < value <= 9300:\n",
    "        ntl_B.append(i)\n",
    "    else :\n",
    "        ntl_C.append(i)\n",
    "\n",
    "CSJntlA = CSJntl[ntl_A]\n",
    "CSJntlB = CSJntl[ntl_B]\n",
    "CSJntlC = CSJntl[ntl_C]\n",
    "\n",
    "rod = ntl_A + ntl_B + ntl_C\n",
    "\n",
    "print(CSJntlA.size())\n",
    "print(CSJntlB.size())\n",
    "print(CSJntlC.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    CSJtfaA = modelA(CSJntlA.to(DEVICE))\n",
    "\n",
    "modelB.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    CSJtfaB = modelB(CSJntlB.to(DEVICE))\n",
    "\n",
    "modelC.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    CSJtfaC = modelC(CSJntlC.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSJtfaO = torch.zeros(CSJtfaA.size(0), 1)\n",
    "device = CSJtfaO.device\n",
    "CSJtfaA = CSJtfaA.to(device)\n",
    "CSJtfaB = CSJtfaB.to(device)\n",
    "CSJtfaC = CSJtfaC.to(device)\n",
    "\n",
    "CSJtfa = torch.cat([CSJtfaA, CSJtfaB, CSJtfaC], dim=0)\n",
    "\n",
    "rod_tensor = torch.tensor(rod, dtype=torch.long)\n",
    "\n",
    "rod_tensor = rod_tensor.unsqueeze(1)\n",
    "\n",
    "CSJtfa_with_rod = torch.cat([CSJtfa, rod_tensor], dim=1)\n",
    "\n",
    "sorted_CSJtfa = CSJtfa_with_rod[CSJtfa_with_rod[:, 1].argsort()]\n",
    "\n",
    "CSJtfa = sorted_CSJtfa[:, 0]\n",
    "CSJtfa = CSJtfa.view(725, 697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181da2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prediction results as a single-band TIFF image\n",
    "tifffile.imwrite('output.tif', CSJtfa.numpy().astype(np.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
